[
  {
    "objectID": "lumos_algorithms.html",
    "href": "lumos_algorithms.html",
    "title": "Software & Algorithms",
    "section": "",
    "text": "In traditional spectral imaging, the hardware dictates the data format. You are forced to capture, store, and process massive 3D data cubes, even if you only need a fraction of that information.\nLumos flips this model.\nOur hardware captures a Diffractogram‚Äîa raw, 2D grayscale image where spatial and spectral information is inextricably encoded by the laws of physics.\nThink of the Diffractogram as a ‚ÄúUniversal Container.‚Äù It is an optically compressed file (typically &lt;1MB) that holds the full complexity of the scene. Because this container is so efficient, it opens up two distinct computational pathways: Direct Inference for modern data-driven machine learning situations, and Reconstruction for traditional spectral inspection of data and analysis.\n\n\nThe following video illustrates the encoding process for a dynamic scene (a butterfly).\n\n\n\n\n\nHyperspectral (Left): The full 3D spectral cube is massive. In the video, you see the ‚Äúunfolded‚Äù bands. We display only 25 spectral bands, but often systems acquire 100+ bands.\nRGB (Top Right): The standard color image we are used to. Low bandwidth, but low spectral information.\nDiffractogram (Bottom Right): This is the actual raw data captured by the Lumos sensor.\n\nResolution: It is a single monochrome frame, identical in pixel count (HD)to a standard monochrome camera.\nInformation Density: Despite being just one frame (smaller than RGB), it encodes the entire spectral complexity of the scene shown in the Hyperspectral view."
  },
  {
    "objectID": "lumos_algorithms.html#the-paradigm-shift-optical-compression",
    "href": "lumos_algorithms.html#the-paradigm-shift-optical-compression",
    "title": "Software & Algorithms",
    "section": "",
    "text": "In traditional spectral imaging, the hardware dictates the data format. You are forced to capture, store, and process massive 3D data cubes, even if you only need a fraction of that information.\nLumos flips this model.\nOur hardware captures a Diffractogram‚Äîa raw, 2D grayscale image where spatial and spectral information is inextricably encoded by the laws of physics.\nThink of the Diffractogram as a ‚ÄúUniversal Container.‚Äù It is an optically compressed file (typically &lt;1MB) that holds the full complexity of the scene. Because this container is so efficient, it opens up two distinct computational pathways: Direct Inference for modern data-driven machine learning situations, and Reconstruction for traditional spectral inspection of data and analysis.\n\n\nThe following video illustrates the encoding process for a dynamic scene (a butterfly).\n\n\n\n\n\nHyperspectral (Left): The full 3D spectral cube is massive. In the video, you see the ‚Äúunfolded‚Äù bands. We display only 25 spectral bands, but often systems acquire 100+ bands.\nRGB (Top Right): The standard color image we are used to. Low bandwidth, but low spectral information.\nDiffractogram (Bottom Right): This is the actual raw data captured by the Lumos sensor.\n\nResolution: It is a single monochrome frame, identical in pixel count (HD)to a standard monochrome camera.\nInformation Density: Despite being just one frame (smaller than RGB), it encodes the entire spectral complexity of the scene shown in the Hyperspectral view."
  },
  {
    "objectID": "lumos_algorithms.html#pathway-a-direct-inference-accessing-spectral-information-without-spectral-cubes",
    "href": "lumos_algorithms.html#pathway-a-direct-inference-accessing-spectral-information-without-spectral-cubes",
    "title": "Software & Algorithms",
    "section": "Pathway A: Direct Inference (accessing spectral information without spectral cubes)",
    "text": "Pathway A: Direct Inference (accessing spectral information without spectral cubes)\n‚ÄúWhy reconstruct a 50GB data cube just to answer a ‚ÄòYes/No‚Äô question?‚Äù\nFor all applications, the goal is not to see the spectrum; the goal is to act on it. A sorting robot needs to know ‚ÄúIs this plastic PVC?‚Äù A satellite needs to know ‚ÄúIs this crop stressed?‚Äù A microscope needs to know ‚ÄúIs this cell cancerous?‚Äù\nLumos enables Direct Inference. We can use modern data-driven machine-learning frameworks and pipelines to use diffractograms directly, mapping the unique texture of the optical code to the desired answer, completely skipping the heavy reconstruction step. Unlike hyperspectral cubes which quickly become too big even for modern GPUs, diffractograms are even smaller than RGB images, making them ideal for edge devices, drones, satellites or on-demand cloud processing.\n\n\n\nDirect Inference Pipeline. Deep Learning models ingest the raw diffractogram and directly output the desired variable (Class, Abundance, etc.), skipping the reconstruction step.\n\n\n\nGamechanging advantages of Direct Inference\n\nLatency:: The small signal allows inference to take place on the edge.\nBandwidth: Ideal for drones and satellites. Transmit the answer (bytes) or the compressed diffractogram (kilobytes) instead of the raw cube (gigabytes).\nEfficiency: Eliminates redundant computation. You don‚Äôt waste energy calculating spectral bands you don‚Äôt need. Storage is efficient and on-demand processing on the cloud becomes a possibility."
  },
  {
    "objectID": "lumos_algorithms.html#pathway-b-traditional-spectral-cube-reconstruction",
    "href": "lumos_algorithms.html#pathway-b-traditional-spectral-cube-reconstruction",
    "title": "Software & Algorithms",
    "section": "Pathway B: Traditional Spectral Cube Reconstruction",
    "text": "Pathway B: Traditional Spectral Cube Reconstruction\nWhen human visualization or detailed scientific analysis is required, we use our Inverse Solver algorithms to decode the diffractogram back into a spectral Cube.\nHowever, unlike traditional cameras where the bands are fixed by the hardware filters (e.g., ‚Äúyou only get these 10 bands‚Äù), Lumos offers Software-Defined Spectral Imaging. This means that we can dynamically select the bands we need after the signal has been acquired. This opens up a world of possibilities for on-demand spectral analysis.\n\n\n\nReconstruction Pipeline. The raw diffractogram is processed by the inverse solver to recover the spatio-spectral 3D cube.\n\n\n\nGamechanging advantages of Direct Inference\n\nA Posteriori Selection: You can decide after taking the picture what spectral data you need.\n\nNeed 4 bands for agriculture? We generate them.\nNeed 25 bands for geology? We generate them from the same raw file.\nNeed to simulate a specific astronomical filter set (e.g., Johnson-Cousins)? We can do that mathematically.\n\nLegacy Compatibility: The output can be saved as standard .HDR or .ENVI files, making Lumos data compatible with decades of existing spectral analysis software."
  },
  {
    "objectID": "lumos_algorithms.html#workflow-comparison",
    "href": "lumos_algorithms.html#workflow-comparison",
    "title": "Software & Algorithms",
    "section": "Workflow Comparison",
    "text": "Workflow Comparison\n\n\n\n\n\n\n\n\nFeature\nTraditional HSI Workflow\nLumos Direct Inference\n\n\n\n\nCapture\nScan scene (slow) or Snapshot (low res)\nHigh-Resolution Snapshot\n\n\nData Size\nHuge (GBs per minute)\nTiny (smaller than RGB)\n\n\nProcessing\nMust process full 3D cube\nProcess only what matters\n\n\nFlexibility\nHardware-fixed bands\nUser-defined bands, easily changed after capture\n\n\nVideo\nTypically not possible\nSame as CMOS sensor (e.g.¬†30 FPS\n\n\n\nSee our algorithms in action ‚Üí"
  },
  {
    "objectID": "validation_and_examples.html",
    "href": "validation_and_examples.html",
    "title": "Validation & Demos",
    "section": "",
    "text": "Our technology transforms standard cameras into high-definition spectral sensors. Below are validation experiments demonstrating how Lumos diffractograms capture invisible decision-making information across industrial, medical, agricultural, and scientific fields.\nLumos technology is versatile. Depending on the application, we can optimize for Speed/Efficiency (Direct Inference) or Scientific Detail (Spectral Reconstruction). Explore the examples below to see both workflows in action."
  },
  {
    "objectID": "validation_and_examples.html#direct-inference-demonstrations",
    "href": "validation_and_examples.html#direct-inference-demonstrations",
    "title": "Validation & Demos",
    "section": "Direct Inference Demonstrations",
    "text": "Direct Inference Demonstrations\nThe Paradigm Shift: Answers, Not Cubes. In these examples, the system bypasses the heavy generation of 3D data cubes. Instead, algorithms extract the answer directly from the raw, compressed Diffractogram. This enables real-time performance on edge devices.\n\nIndustrial SortingAgricultural MonitoringOrbital Edge AI\n\n\nThe Challenge: In manufacturing, contaminants often look identical to the product.\nThe Demo: We imaged a stream of mixed plastic pellets. Half were pure, half were ‚Äútainted‚Äù with a clear varnish invisible to the naked eye.\n\nRGB Camera: Failed. Pure and tainted look identical.\nHyperspectral Camera: Succeeded, but required 15.87 MB per frame.\nLumos Camera: Succeeded with just 0.37 MB per frame (43:1 compression).\n\n\n\n\n\nReal-time classification of moving pellets using &lt;1MB of data.\n\n\n\n\nThe Challenge: Traditional ‚ÄúPush-broom‚Äù scanners require objects to be perfectly still or moving on a flat belt. They fail with tumbling, rotating 3D objects.\nThe Demo: We captured an avocado rotating on a turntable every 10 minutes for 7 days.\n\nSnapshot Advantage: Because Lumos captures the whole scene instantly (\\(x, y, \\lambda\\)), it is immune to the motion artifacts (shearing/wobble) that plague line-scanners.\n4D Data: We successfully mapped the ripening process in both space (3D surface) and time.\n\n\n\n\n\nSnapshot acquisition allows for distortion-free imaging of rotating 3D objects.\n\n\n\n\nThe Challenge: Satellites generate Terabytes of data, but radio downlinks are slow.\nThe Demo: We simulated a Lumos sensor in orbit using Landsat 8 data.\n\nTask: Estimate vegetation abundance (0-100%) directly from the compressed diffractogram.\nResult: The Deep Neural Network mapped the monochromatic diffractogram directly to an abundance map. The data required was ~1000x smaller than the raw hyperspectral file, solving the downlink bottleneck.\n\n\n\n\n\n\n\n\nInput: Compressed Diffractogram\n\n\n\n\n\n\n\nPrediction: Estimated Abundance\n\n\n\n\n\n\n\nGround Truth: Full Spectrum"
  },
  {
    "objectID": "validation_and_examples.html#applications-using-spectral-reconstructions-from-diffractogram-data",
    "href": "validation_and_examples.html#applications-using-spectral-reconstructions-from-diffractogram-data",
    "title": "Validation & Demos",
    "section": "Applications using Spectral Reconstructions from Diffractogram data",
    "text": "Applications using Spectral Reconstructions from Diffractogram data\nThe Traditional View: High-Fidelity Cubes. For scientific analysis, medical diagnostics, or legacy software compatibility, we reconstruct the full 3D Hyperspectral Cube.\n\nMedical ImagingSmart AgAstronomy\n\n\nThe Challenge: Distinguishing healthy tissue from critical structures without dyes.\nThe Demo: We imaged ex-vivo chicken tissue containing both lung and trachea. While visually similar (pink tissue), they have distinct chemical signatures.\n\nMethod: Reconstructed spectral cube + Linear Discriminant Analysis (LDA).\nResult: The system successfully segmented the image, clearly differentiating the trachea (red) from the lungs (green) based solely on spectral fingerprints.\n\n\n\n\n\nTissue Classification. (A) RGB image with classification overlay. (B) Spectral curves showing distinct signatures for Lung vs.¬†Trachea.\n\n\n\n\n\nThe Challenge: You only see rot after it‚Äôs too late. The Demo: We monitored 21 strawberries continuously over 8 days. * Spectral Evolution: The sensor detected a decline in the ‚ÄúGreen Peak‚Äù (chlorophyll) and shifts in the ‚ÄúRed Edge‚Äù days before visual spoilage. * Result: We could predict the exact age of the fruit and classify ‚ÄúFresh‚Äù vs.¬†‚ÄúSpoiled‚Äù with high accuracy before visible signs appeared.\n\n\n\n\nStrawberry Aging. (Left) Spectral decay over 8 days. (Right) ROC curve showing high classification accuracy.\n\n\n\n\n\nThe Challenge: Changing physical filters on a telescope is slow and discards light. The Demo: We simulated a stellar field and captured all data in a single snapshot. * A Posteriori Selection: Instead of a physical filter wheel, we applied the standard Johnson-Cousins (BVRI) transmission curves mathematically after capture. * Result: We recovered accurate photometric ratios for the stars, proving that one Lumos sensor can replace a multi-filter mechanical assembly.\n\n\n\n\nSynthetic Stellar Imaging. (Top) Raw Diffractograms. (Bottom) Accurate recovery of B, V, R, and I bands from a single exposure."
  },
  {
    "objectID": "validation_and_examples.html#scientific-validation-optica-2025",
    "href": "validation_and_examples.html#scientific-validation-optica-2025",
    "title": "Validation & Demos",
    "section": "Scientific Validation (Optica 2025)",
    "text": "Scientific Validation (Optica 2025)\nOur technology is backed by rigorous peer-reviewed research.\nPaper: Apratim Majumder, Monjurul Meem, Fernando Gonzalez del Cueto, Fernando Guevara Vasquez, Syed N. Qadri, Freddie Santiago, and Rajesh Menon, ‚ÄúHigh-definition (HD) snapshot diffractive computational spectral imaging and inferencing,‚Äù Optica 12, 1539-1547 (2025). Read Paper\nKey Performance Metrics:\n\nTrue HD Resolution: Unlike competitors limited to VGA (\\(640 \\times 480\\)), we demonstrated reconstruction at 1304 √ó 744 pixels (~1 Megapixel).\nSpectral Fidelity: We consistently achieve &lt;15% spectral reproduction error, sufficient for the vast majority of discrimination tasks.\nBroadband & Narrowband: Validated performance on both continuous broad spectra (natural light) and discrete narrow bands (lasers/LEDs)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The End of the RGB Era",
    "section": "",
    "text": "High Definition. Low Cost. Snapshot.Historically, you could only pick two. Lumos delivers all three.We democratize access to material intelligence by combining nanophotonics with physics-aware AI."
  },
  {
    "objectID": "index.html#the-data-bottleneck",
    "href": "index.html#the-data-bottleneck",
    "title": "The End of the RGB Era",
    "section": "The Data Bottleneck",
    "text": "The Data Bottleneck\nHyperspectral imaging has long been trapped by the ‚ÄúDimensionality Curse.‚Äù See for yourself how quickly raw data becomes unmanageable‚Äîand how Lumos solves it.\n\n    \n        \n            Data Explosion Calculator\n            \n            \n                \n                    Image Resolution\n                    \n                        128 x 128 px\n                        256 x 256 px\n                        512 x 512 px\n                        1024 x 1024 px\n                    \n                \n                \n                \n                    Video Duration\n                    \n                        30 Seconds\n                        1 Minute\n                        5 Minutes\n                        30 Minutes\n                    \n                \n            \n        \n\n        \n            \n\n\n\nFormat\nChannels in image\nSpectral Bands\nBits per pixel\nRelative Size\nSingle Frame\nVideo Stream\n\n\n\n\n\n\n        \n        \n            * Note on Lumos Bands: Unlike traditional sensors that capture specific, pre-defined wavebands (which increases data size linearly), Lumos captures the entire continuous light field compressed into a single 16-bit diffractogram. We do not capture \"bands\"; we capture the physics of light itself. Any number of bands can be mathematically reconstructed, if desired, from this single frame later, meaning our raw data footprint remains constant regardless of spectral resolution. However, we have shown that the intrinsic dimensionality of the spectral space is at least 25."
  },
  {
    "objectID": "index.html#choose-your-journey",
    "href": "index.html#choose-your-journey",
    "title": "The End of the RGB Era",
    "section": "Choose Your Journey",
    "text": "Choose Your Journey\n\n\n\n\nüåà\n\n\nNew to Spectral Imaging?\nDiscover how the ‚ÄúChemistry of Light‚Äù is revolutionizing healthcare, agriculture, and robotics.\nExplore the Power of Light ‚Üí\n\n\n\n\n\n\nüöß\n\n\nWhy hasn‚Äôt it scaled?\nLearn about the three engineering bottlenecks‚ÄîCost, Mechanics, and Data‚Äîthat have kept HSI in the lab.\nUnderstand the Barriers ‚Üí\n\n\n\n\n\n\nüí°\n\n\nThe Lumos Solution\nSee how we combine Nanofabrication and Computational Imaging to break the Iron Triangle.\nDiscover the Tech ‚Üí"
  },
  {
    "objectID": "about_us.html",
    "href": "about_us.html",
    "title": "About Us",
    "section": "",
    "text": "Lumos Imaging is a spin-off from the University of Utah. We sit at the intersection of Nanophotonics, Applied Mathematics, and Commercial Product Strategy.\nOur mission is to translate the complex physics of diffractive optics into scalable, robust imaging solutions that solve real-world industrial problems."
  },
  {
    "objectID": "about_us.html#leadership-team",
    "href": "about_us.html#leadership-team",
    "title": "About Us",
    "section": "Leadership Team",
    "text": "Leadership Team\n\n\n\n\n\n\nRajesh Menon \n\nRajesh is a Professor at the University of Utah with a PhD in Diffractive Optics from MIT. He is a Fellow of OSA and SPIE, and a serial entrepreneur having founded LumArray and PointSpectrum. With over 100 publications and 40+ patents in nanophotonics and lithography, Rajesh brings deep technical expertise and a proven track record of translating research into commercial products.\n\n\n\n\n\n\n\n\nLaurent Node-Langlois \n\nLaurent brings over 25 years of high-tech Product Management experience. Formerly in executive leadership at GE Healthcare, where he led Medical Imaging and Surgical Navigation products, his extensive P&L responsibility includes guiding deep-tech products from R&D through market exit, bringing critical commercial execution expertise to the team.\n\n\n\n\n\n\n\n\nFernando Guevara V√°squez, PhD \n\nFernando holds a PhD in Computational & Applied Mathematics from Rice University. His research in mathematical modeling of wave propagation and inverse problems has been fundamental to developing the theoretical foundations of Lumos‚Äôs diffractive computational imaging technology. His work on optimization algorithms enables the robust reconstruction of spectral information from optically encoded data.\n\n\n\n\n\n\n\n\nFernando Gonz√°lez del Cueto, PhD \n\nFernando holds a PhD in Computational & Applied Mathematics from Rice University. His expertise in imaging science and numerical optimization has been instrumental in developing Lumos‚Äôs reconstruction algorithms and neural network architectures. His work bridges the gap between computational theory and practical implementation, creating the software foundation that makes Lumos technology viable."
  },
  {
    "objectID": "about_us.html#intellectual-property-portfolio",
    "href": "about_us.html#intellectual-property-portfolio",
    "title": "About Us",
    "section": "Intellectual Property Portfolio",
    "text": "Intellectual Property Portfolio\nLumos Imaging holds an exclusive, worldwide license to a comprehensive patent portfolio developed at the University of Utah. This IP protects the entire vertical stack‚Äîfrom the nanofabrication of the optic to the algorithmic reconstruction of the image‚Äîcreating a strong defensive moat.\n\nPatent Portfolio\n\n\n\n\n\n\n\nPatent ID\nTitle\nScope of Protection\n\n\n\n\nUS 9,723,230\nMulti-spectral imaging with diffractive optics\nHardware Architecture: Protects the physical combination of a sensor array and a diffractive element in the optical path.\n\n\nUS 11,300,449\nImaging device creating a spatially coded image\nData Format: Protects the ‚ÄúDiffractogram‚Äù data structure and the forward model logic.\n\n\nUS 8,953,239\nNanophotonic scattering structure\nDesign Algorithms: Protects the computational methods used to optimize the geometry of the DFA for specific spectral responses.\n\n\nUS 10,395,134\nExtraction of spectral information\nSoftware/Reconstruction: Protects the inverse problem algorithms used to recover the spectral cube from the raw data.\n\n\nUS 12,052,518\nMulti-modal computational imaging via metasurfaces\nAdvanced Modalities: Protects the extension of the DFA technology to other imaging modalities like Lightfield and Depth sensing."
  },
  {
    "objectID": "about_us.html#commercialization-strategic-partnerships",
    "href": "about_us.html#commercialization-strategic-partnerships",
    "title": "About Us",
    "section": "Commercialization & Strategic Partnerships",
    "text": "Commercialization & Strategic Partnerships\nWe are moving from R&D to Scale.\nLumos Imaging is actively seeking strategic partners to help us scale our technology."
  },
  {
    "objectID": "about_us.html#contact-us",
    "href": "about_us.html#contact-us",
    "title": "About Us",
    "section": "Contact Us",
    "text": "Contact Us\n\n\n\n\nEmail Us\n\n\nInterested in partnering? Reach out directly.\n\ninfo@lumosimaging.com\n\n\n\n\n\nFollow Us\n\n\nKeep up with our latest technology updates.\n\n  Lumos Imaging"
  },
  {
    "objectID": "barriers_to_HSI_adoption.html",
    "href": "barriers_to_HSI_adoption.html",
    "title": "Current Barriers to Wider Adoption of Spectral Imaging",
    "section": "",
    "text": "Traditional high-performance hyperspectral cameras rely on complex optical trains. This translates into high costs in hardware: depending on the specs, the cameras can range typically from $30k to $150k USD!\n\nComponents: These systems require precision slits, collimating mirrors, diffraction gratings, and re-focusing optics.\nAlignment: The optical path requires sub-micron alignment stability across a wide temperature range.\nConsequence: This complexity drives high unit costs ($20k‚Äì$150k USD) and makes miniaturization extremely difficult. It effectively limits HSI to high-budget research labs or military assets.\n\n\nLumos Solution: We replace complex optical trains with a single, nanofabricated diffractive optical element placed on top of an inexpensive, off-the-shelf CMOS camera. By leveraging semiconductor manufacturing techniques (Nano-Imprint Lithography), we drive the cost structure down to a level compatible with mass-market sensors."
  },
  {
    "objectID": "barriers_to_HSI_adoption.html#the-optical-complexity-barrier-cost",
    "href": "barriers_to_HSI_adoption.html#the-optical-complexity-barrier-cost",
    "title": "Current Barriers to Wider Adoption of Spectral Imaging",
    "section": "",
    "text": "Traditional high-performance hyperspectral cameras rely on complex optical trains. This translates into high costs in hardware: depending on the specs, the cameras can range typically from $30k to $150k USD!\n\nComponents: These systems require precision slits, collimating mirrors, diffraction gratings, and re-focusing optics.\nAlignment: The optical path requires sub-micron alignment stability across a wide temperature range.\nConsequence: This complexity drives high unit costs ($20k‚Äì$150k USD) and makes miniaturization extremely difficult. It effectively limits HSI to high-budget research labs or military assets.\n\n\nLumos Solution: We replace complex optical trains with a single, nanofabricated diffractive optical element placed on top of an inexpensive, off-the-shelf CMOS camera. By leveraging semiconductor manufacturing techniques (Nano-Imprint Lithography), we drive the cost structure down to a level compatible with mass-market sensors."
  },
  {
    "objectID": "barriers_to_HSI_adoption.html#the-acquisition-constraint-mechanics",
    "href": "barriers_to_HSI_adoption.html#the-acquisition-constraint-mechanics",
    "title": "Current Barriers to Wider Adoption of Spectral Imaging",
    "section": "2. The Acquisition Constraint (Mechanics)",
    "text": "2. The Acquisition Constraint (Mechanics)\n The dominant architecture for high-resolution HSI is the Push-broom (Line-scan) sensor.\n\nMechanism: The sensor captures one line at a time. To build a 3D spectral volume (\\(x, y, \\lambda\\)), the image must be stitched afterwards.\nFailure Modes:\n\nVibration: Any unmodeled vibration (e.g., from a drone or vehicle) results in ‚Äúwobbly‚Äù images that are geometrically distorted. Rectification can be a challenge.\nDynamic Scenes: If objects in the scene move during the scan, they become sheared or artifacted.\n\nConsequence: Push-broom sensors are notoriously difficult to deploy on UAVs, handheld devices, or in dynamic industrial settings.\n\n\nLumos Solution: We utilize a Snapshot architecture. We capture the full spatial-spectral volume \\((x, y, \\lambda)\\) in a single integration period. This makes the system immune to vibration and motion artifacts, treating spectral video just like standard video."
  },
  {
    "objectID": "barriers_to_HSI_adoption.html#the-dimensionality-curse-data",
    "href": "barriers_to_HSI_adoption.html#the-dimensionality-curse-data",
    "title": "Current Barriers to Wider Adoption of Spectral Imaging",
    "section": "3. The Dimensionality Curse (Data)",
    "text": "3. The Dimensionality Curse (Data)\n One understated barrier to widespread spectral adoption is the massive size of hyperpectral volumes. An uncompressed image array must be stored and processed for every spectral band.\nFor a standard \\(512 \\times 512\\) pixel sensor, the uncompressed data requirements vary drastically:\n\n\n\n\n\n\n\n\n\n\n\nArchitecture\nChannels\nBit Depth\nFrame Size\n1000-Frame Video\n\n\n\n\nStandard RGB\n3\n8-bit\n0.75 MB\n0.75 GB\n\n\nCommon HSI\n25\n16-bit\n12.5 MB\n12.5 GB\n\n\nHigh-Res HSI\n120\n16-bit\n60.0 MB\n60.0 GB\n\n\nLumos Diffractogram\n1\n12-bit\n0.50 MB\n0.50 GB\n\n\n\n\nConsequences:\n\nSatellite Downlinks: An earth-observation satellite creates terabytes of data. Downlinking this via limited radio bandwidth is prohibitively expensive or slow. Operators often discard 90% of the data. Ironically, analysts sometimes discard a lot of these data which is unnecessary for some applications since they‚Äôre only interested in a few wavelengths, or end up combining them to an end product that is much smaller than the original datasets. Spectral data cubes are not only huge, but information is encoded very redundantly in them, making it inefficient!\nDrone Telemetry: A UAV many not be able transmit live spectral video to a ground station because the bitrate can easily exceed standard wireless protocols.\nEdge Compute: Embedded processors cannot reconstruct and analyze heavy 3D data cubes in real-time (60 FPS).\nArchival: Storing petabytes of raw hyperspectral cubes for historical analysis is very expensive.\n\n\nLumos Solution: Analog Optical Compression. Spectral ‚Äúcubes‚Äù are known to be highly redundant and users of these data often throw away most of it. The Lumos Camera captures instead a lean signal that encodes spatial and spectral content in a very information-dense manner. Not to be confused with software compression, this represents a massive reduction in raw data volume, enabling the use in modern machine learning pipelines, efficient transmission from space, cloud archival and retrieval, and real-time processing for edge AI applications. Even HD video streams are possible, whereas traditional approaches are still unfeasible.\n\nLearn more about the Lumos Solution"
  },
  {
    "objectID": "lumos_solution.html",
    "href": "lumos_solution.html",
    "title": "The Lumos Hardware",
    "section": "",
    "text": "To break the barriers of cost and complexity, we didn‚Äôt just shrink a traditional spectrometer. We reimagined the physics of imaging.\nLumos employs a Computational Imaging approach. We shift the burden of complexity from bulky hardware (prisms, mirrors, moving parts) to sophisticated software. This allows us to use simple, robust, and scalable components to achieve performance that was previously impossible at this price point."
  },
  {
    "objectID": "lumos_solution.html#the-hardware-software-co-design",
    "href": "lumos_solution.html#the-hardware-software-co-design",
    "title": "The Lumos Hardware",
    "section": "",
    "text": "To break the barriers of cost and complexity, we didn‚Äôt just shrink a traditional spectrometer. We reimagined the physics of imaging.\nLumos employs a Computational Imaging approach. We shift the burden of complexity from bulky hardware (prisms, mirrors, moving parts) to sophisticated software. This allows us to use simple, robust, and scalable components to achieve performance that was previously impossible at this price point."
  },
  {
    "objectID": "lumos_solution.html#system-architecture-a-drop-in-revolution",
    "href": "lumos_solution.html#system-architecture-a-drop-in-revolution",
    "title": "The Lumos Hardware",
    "section": "System Architecture: A Drop-In Revolution",
    "text": "System Architecture: A Drop-In Revolution\nThe Lumos camera looks, acts, and connects just like a standard machine vision camera. There are no slit-scanners, no heavy gimbals, and no fragile alignment mechanisms.\nThe physical stack consists of three layers:\n\nStandard Optics: Compatible with off-the-shelf C-Mount lenses.\nThe Diffractive Filter Array (DFA): Our core IP. A transparent, nanofabricated optical element bonded directly to the sensor.\nCommodity Sensors: We leverage the massive R&D economies of the mobile phone and industrial sensor markets.\n\nVis-NIR: Standard high-resolution CMOS sensors (e.g., Sony IMX).\nSWIR: InGaAs sensors for industrial sorting.\n\n\n\n\n\nThe Optical Architecture. Light from the scene passes through the transparent DFA placed ~50¬µm from the sensor pixels. The DFA creates a micro-scale diffraction pattern that encodes spectral data into the image."
  },
  {
    "objectID": "lumos_solution.html#innovative-wafer-level-nanofabrication",
    "href": "lumos_solution.html#innovative-wafer-level-nanofabrication",
    "title": "The Lumos Hardware",
    "section": "Innovative Wafer-Level Nanofabrication",
    "text": "Innovative Wafer-Level Nanofabrication\nTraditional hyperspectral cameras are expensive because they are built like watches: precision glass components, hand-assembled and aligned.\nLumos cameras are built like microchips.\nWe utilize Nano-Imprint Lithography (NIL) to fabricate the Diffractive Filter Arrays. * Scalability: We print optics on standard wafers. Thousands of DFAs are produced in a single step. * Cost: This semiconductor-based approach drives the unit cost down by orders of magnitude compared to traditional interference filters or prisms. * Robustness: The DFA is a solid-state surface relief structure. It is immune to vibration and thermal drift.\n\n\n\nMicroscopic view of the DFA. These micro-scale ridges diffract light based on wavelength, encoding the spectrum into the spatial domain."
  },
  {
    "objectID": "lumos_solution.html#breaking-the-resolution-limit-true-hd",
    "href": "lumos_solution.html#breaking-the-resolution-limit-true-hd",
    "title": "The Lumos Hardware",
    "section": "Breaking the Resolution Limit (True HD)",
    "text": "Breaking the Resolution Limit (True HD)\nOne of the greatest compromises in snapshot spectral imaging has always been resolution. Traditional ‚ÄúMosaic‚Äù cameras (using pixel-level filters) trade spatial resolution for spectral bands, often resulting in grainy, low-fidelity images (e.g., \\(400 \\times 400\\) pixels).\nLumos delivers True High Definition.\nBecause our diffractive encoding is continuous and distributed, we preserve high-frequency spatial details that other systems lose. As validated in our recent Optica publication, we achieve:\n\n~1 Megapixel Resolution: \\(1304 \\times 744\\) spatial pixels.\nFull Spectral Context: 25+ bands across the Vis-NIR range.\n\nThis allows Lumos sensors to be used in applications requiring fine detail, such as detecting small defects on a production line or identifying features from high-altitude drones."
  },
  {
    "objectID": "lumos_solution.html#comparing-the-architectures",
    "href": "lumos_solution.html#comparing-the-architectures",
    "title": "The Lumos Hardware",
    "section": "Comparing the Architectures",
    "text": "Comparing the Architectures\n\n\n\n\n\n\n\n\n\nFeature\nTraditional Push-broom\nSnapshot Mosaic\nLumos Diffractive\n\n\n\n\nMechanics\nLine-Scan (Needs motion)\nSnapshot\nSnapshot\n\n\nResolution\nHigh (Linear)\nLow (Spatial decimation)\nHigh (HD Reconstructed)\n\n\nLight Throughput\nLow (Slit-limited)\nLow (Absorptive Filters)\nHigh (Transparent Phase Optic)\n\n\nData Output\nMassive Raw Cube\nMassive Raw Cube\nCompressed Diffractogram"
  },
  {
    "objectID": "lumos_solution.html#the-result-the-diffractogram",
    "href": "lumos_solution.html#the-result-the-diffractogram",
    "title": "The Lumos Hardware",
    "section": "The Result: The Diffractogram",
    "text": "The Result: The Diffractogram\nThe output of this hardware is not a heavy 3D data cube, nor is it a simple RGB image. It is a Diffractogram‚Äîa raw, optically compressed signal that contains the full complexity of the scene in a highly efficient format.\nThis signal is the key to our ‚ÄúDirect Inference‚Äù capabilities.\nSee how we decode the light ‚Üí"
  },
  {
    "objectID": "why_spectral_imaging.html",
    "href": "why_spectral_imaging.html",
    "title": "The Untapped Power of Light",
    "section": "",
    "text": "Most of the physical world is invisible to our eyes and to standard RGB cameras.\nConventional imaging systems‚Äîwhether in your phone or an industrial robot‚Äîare designed to mimic Human Vision. They capture light in three broad channels: Red, Green, and Blue (RGB). While this is perfect for photography, it is a sub-optimal approach for sensing.\nBy averaging light into these three broad buckets, RGB cameras throw away 99% of the physical information contained in the spectrum. This creates a dangerous ambiguity: a red plastic apple and a real red apple may look very similar to an RGB sensor, yet they are chemically distinct."
  },
  {
    "objectID": "why_spectral_imaging.html#the-blind-spot-in-modern-sensing",
    "href": "why_spectral_imaging.html#the-blind-spot-in-modern-sensing",
    "title": "The Untapped Power of Light",
    "section": "",
    "text": "Most of the physical world is invisible to our eyes and to standard RGB cameras.\nConventional imaging systems‚Äîwhether in your phone or an industrial robot‚Äîare designed to mimic Human Vision. They capture light in three broad channels: Red, Green, and Blue (RGB). While this is perfect for photography, it is a sub-optimal approach for sensing.\nBy averaging light into these three broad buckets, RGB cameras throw away 99% of the physical information contained in the spectrum. This creates a dangerous ambiguity: a red plastic apple and a real red apple may look very similar to an RGB sensor, yet they are chemically distinct."
  },
  {
    "objectID": "why_spectral_imaging.html#the-chemistry-of-light",
    "href": "why_spectral_imaging.html#the-chemistry-of-light",
    "title": "The Untapped Power of Light",
    "section": "The ‚ÄúChemistry‚Äù of Light",
    "text": "The ‚ÄúChemistry‚Äù of Light\n\nEvery molecule in the universe interacts with light in a unique way, absorbing and reflecting specific wavelengths of the incoming light to create a distinct spectral ‚Äúfingerprint.‚Äù\n\nChlorophyll has a specific signature in the Near-Infrared.\nHemoglobin changes its absorption based on oxygenation levels.\nPolymers (plastics) have distinct vibrational overtones in the infrared.\n\nThis allows us to infer the composition of the objects being observed.\nBy resolving light into dozens or hundreds of narrow bands, we don‚Äôt just take a picture of an object; we can identify what it is made of, detect contaminants, and many other tasks that are otherwise very hard or impossible.\n\n\n\n\n\n\n\nNoteVisualizing the Difference\n\n\n\n![PLACEHOLDER: Schematic comparing RGB (three broad humps) vs Spectral (many narrow spikes). Description: Show how two different materials might look the same in RGB (same area under the curve) but have totally different spectral spikes.]"
  },
  {
    "objectID": "why_spectral_imaging.html#solving-the-invisible-problems",
    "href": "why_spectral_imaging.html#solving-the-invisible-problems",
    "title": "The Untapped Power of Light",
    "section": "Solving the ‚ÄúInvisible‚Äù Problems",
    "text": "Solving the ‚ÄúInvisible‚Äù Problems\nSpectral imaging transforms vague visual data into Actionable Intelligence. It is already revolutionizing high-value industries by solving problems that standard computer vision simply cannot touch. A few examples:\n\nPrecision Agriculture (Agro-tech)\n Pre-Symptomatic Detection Plants react to stress chemically long before they wilt or turn yellow.\n\nWater Stress: Detect canopy moisture levels to optimize irrigation.\nDisease: Identify fungal infections days before they are visible to the human eye.\nRipeness: Grade fruit based on internal sugar content (Brix) rather than external color.\n\n\n\nHealthcare\n\nSurgeons currently rely on invasive biopsies or chemical dyes to identify tissues. HSI provides a non-invasive alternative.\n\nTumor Margins: Distinguish between healthy cells and cancerous tissue in real-time during surgery.\nOxygenation: Map tissue perfusion to assess wound healing or diabetic ulcers without touching the patient.\n\n\n\nIndustrial Sorting\n In a recycling stream, clear PVC and clear PET plastic look identical.\n\nRecycling: Sort plastics by chemical polymer type to ensure purity.\nQuality Control: Detect invisible foreign contaminants (like clear varnish or moisture pockets) in food and pharmaceutical production lines."
  },
  {
    "objectID": "why_spectral_imaging.html#a-growing-economic-engine",
    "href": "why_spectral_imaging.html#a-growing-economic-engine",
    "title": "The Untapped Power of Light",
    "section": "A Growing Economic Engine",
    "text": "A Growing Economic Engine\nThe value of this ‚ÄúMaterial Intelligence‚Äù is driving explosive market growth. As industries automate, the need for sensors that can discern chemistry is becoming critical.\n\nMarket Forecast: The Global Hyperspectral Imaging market is valued at $219.69M in 2026 and is expected to reach $772.74M by 2035, growing at a 15% CAGR.\n‚Äî Source: MarketGrowthReports (2025)"
  },
  {
    "objectID": "why_spectral_imaging.html#the-adoption-gap",
    "href": "why_spectral_imaging.html#the-adoption-gap",
    "title": "The Untapped Power of Light",
    "section": "The Adoption Gap",
    "text": "The Adoption Gap\nIf Spectral Imaging provides such a massive advantage over RGB, why isn‚Äôt it on every drone, tractor, and production line?\nWhy is it still largely confined to research labs?\nThe answer lies in Three Engineering Barriers that have historically made the technology too expensive, too fragile, and too data-heavy for the real world.\nUncover the Barriers ‚Üí"
  }
]