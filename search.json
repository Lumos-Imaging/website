[
  {
    "objectID": "why_spectral_imaging.html",
    "href": "why_spectral_imaging.html",
    "title": "The Untapped Power of Light",
    "section": "",
    "text": "Most of the physical world is invisible to our eyes and to standard RGB cameras.\nConventional imaging systems‚Äîwhether in your phone or an industrial robot‚Äîare designed to mimic Human Vision. They capture light in three broad channels: Red, Green, and Blue (RGB). While this is perfect for photography, it is a sub-optimal approach for sensing.\nBy averaging light into these three broad buckets, RGB cameras throw away 99% of the physical information contained in the spectrum. This creates a dangerous ambiguity: a red plastic apple and a real red apple may look very similar to an RGB sensor, yet they are chemically distinct."
  },
  {
    "objectID": "why_spectral_imaging.html#the-chemistry-of-light",
    "href": "why_spectral_imaging.html#the-chemistry-of-light",
    "title": "The Untapped Power of Light",
    "section": "The ‚ÄúChemistry‚Äù of Light",
    "text": "The ‚ÄúChemistry‚Äù of Light\n\nEvery molecule in the universe interacts with light in a unique way, absorbing and reflecting specific wavelengths of the incoming light to create a distinct spectral ‚Äúfingerprint.‚Äù\n\nChlorophyll has a specific signature in the Near-Infrared.\nHemoglobin changes its absorption based on oxygenation levels.\nPolymers (plastics) have distinct vibrational overtones in the infrared.\n\nThis allows us to infer the composition of the objects being observed.\nBy resolving light into dozens or hundreds of narrow bands, we don‚Äôt just take a picture of an object; we can identify what it is made of, detect contaminants, and many other tasks that are otherwise very hard or impossible."
  },
  {
    "objectID": "why_spectral_imaging.html#solving-the-invisible-problems",
    "href": "why_spectral_imaging.html#solving-the-invisible-problems",
    "title": "The Untapped Power of Light",
    "section": "Solving the ‚ÄúInvisible‚Äù Problems",
    "text": "Solving the ‚ÄúInvisible‚Äù Problems\nSpectral imaging transforms vague visual data into Actionable Intelligence. It is already revolutionizing high-value industries by solving problems that standard computer vision simply cannot touch. A few examples:\n\nPrecision Agriculture (Agro-tech)\n Pre-Symptomatic Detection Plants react to stress chemically long before they wilt or turn yellow.\n\nWater Stress: Detect canopy moisture levels to optimize irrigation.\nDisease: Identify fungal infections days before they are visible to the human eye.\nRipeness: Grade fruit based on internal sugar content (Brix) rather than external color.\n\n\n\nHealthcare\n\nSurgeons currently rely on invasive biopsies or chemical dyes to identify tissues. HSI provides a non-invasive alternative.\n\nTumor Margins: Distinguish between healthy cells and cancerous tissue in real-time during surgery.\nOxygenation: Map tissue perfusion to assess wound healing or diabetic ulcers without touching the patient.\n\n\n\nIndustrial Sorting\n In a recycling stream, clear PVC and clear PET plastic look identical.\n\nRecycling: Sort plastics by chemical polymer type to ensure purity.\nQuality Control: Detect invisible foreign contaminants (like clear varnish or moisture pockets) in food and pharmaceutical production lines."
  },
  {
    "objectID": "why_spectral_imaging.html#a-growing-economic-engine",
    "href": "why_spectral_imaging.html#a-growing-economic-engine",
    "title": "The Untapped Power of Light",
    "section": "A Growing Economic Engine",
    "text": "A Growing Economic Engine\nThe value of this ‚ÄúMaterial Intelligence‚Äù is driving explosive market growth. As industries automate, the need for sensors that can discern chemistry is becoming critical.\n\nMarket Forecast: The Global Hyperspectral Imaging market is valued at $219.69M in 2026 and is expected to reach $772.74M by 2035, growing at a 15% CAGR.\n‚Äî Source: MarketGrowthReports (2025)"
  },
  {
    "objectID": "why_spectral_imaging.html#the-adoption-gap",
    "href": "why_spectral_imaging.html#the-adoption-gap",
    "title": "The Untapped Power of Light",
    "section": "The Adoption Gap",
    "text": "The Adoption Gap\nIf Spectral Imaging provides such a massive advantage over RGB, why isn‚Äôt it on every drone, tractor, and production line?\nWhy is it still largely confined to research labs or niche applications?\nThe answer lies in Three Barriers that have historically made the technology too expensive, too fragile, and too data-heavy for the real world.\nUncover the Barriers ‚Üí"
  },
  {
    "objectID": "lumos_solution.html",
    "href": "lumos_solution.html",
    "title": "The Lumos Hardware",
    "section": "",
    "text": "To break the barriers of cost and complexity, we didn‚Äôt just shrink a traditional spectrometer. We reimagined the physics of imaging.\nLumos employs a Computational Imaging approach. We shift the burden of complexity from bulky hardware (prisms, mirrors, moving parts) to sophisticated software. This allows us to use simple, robust, and scalable components to achieve performance that was previously impossible at this price point."
  },
  {
    "objectID": "lumos_solution.html#the-hardware-software-co-design",
    "href": "lumos_solution.html#the-hardware-software-co-design",
    "title": "The Lumos Hardware",
    "section": "",
    "text": "To break the barriers of cost and complexity, we didn‚Äôt just shrink a traditional spectrometer. We reimagined the physics of imaging.\nLumos employs a Computational Imaging approach. We shift the burden of complexity from bulky hardware (prisms, mirrors, moving parts) to sophisticated software. This allows us to use simple, robust, and scalable components to achieve performance that was previously impossible at this price point."
  },
  {
    "objectID": "lumos_solution.html#system-architecture-a-drop-in-revolution",
    "href": "lumos_solution.html#system-architecture-a-drop-in-revolution",
    "title": "The Lumos Hardware",
    "section": "System Architecture: A Drop-In Revolution",
    "text": "System Architecture: A Drop-In Revolution\nThe Lumos camera looks, acts, and connects just like a standard machine vision camera. There are no slit-scanners, no heavy gimbals, and no fragile alignment mechanisms.\nThe physical stack consists of three layers:\n\nStandard Optics: Compatible with off-the-shelf C-Mount lenses.\nThe Diffractive Filter Array (DFA): Our core IP. A transparent, nanofabricated optical element bonded directly to the sensor.\nCommodity Sensors: We leverage the massive R&D economies of the mobile phone and industrial sensor markets.\n\nVis-NIR: Standard high-resolution CMOS sensors (e.g., Sony IMX).\nSWIR: InGaAs sensors for industrial sorting.\n\n\n\n\n\nThe Optical Architecture. Light from the scene passes through the transparent DFA placed ~50¬µm from the sensor pixels. The DFA creates a micro-scale diffraction pattern that encodes spectral data into the image."
  },
  {
    "objectID": "lumos_solution.html#innovative-wafer-level-nanofabrication",
    "href": "lumos_solution.html#innovative-wafer-level-nanofabrication",
    "title": "The Lumos Hardware",
    "section": "Innovative Wafer-Level Nanofabrication",
    "text": "Innovative Wafer-Level Nanofabrication\nTraditional hyperspectral cameras are expensive because they are built like watches: precision glass components, hand-assembled and aligned.\nLumos cameras are built like microchips.\nWe utilize Nano-Imprint Lithography (NIL) to fabricate the Diffractive Filter Arrays. * Scalability: We print optics on standard wafers. Thousands of DFAs are produced in a single step. * Cost: This semiconductor-based approach drives the unit cost down by orders of magnitude compared to traditional interference filters or prisms. * Robustness: The DFA is a solid-state surface relief structure. It is immune to vibration and thermal drift.\n\n\n\nMicroscopic view of the DFA. These micro-scale ridges diffract light based on wavelength, encoding the spectrum into the spatial domain."
  },
  {
    "objectID": "lumos_solution.html#breaking-the-resolution-limit-true-hd",
    "href": "lumos_solution.html#breaking-the-resolution-limit-true-hd",
    "title": "The Lumos Hardware",
    "section": "Breaking the Resolution Limit (True HD)",
    "text": "Breaking the Resolution Limit (True HD)\nOne of the greatest compromises in snapshot spectral imaging has always been resolution. Traditional ‚ÄúMosaic‚Äù cameras (using pixel-level filters) trade spatial resolution for spectral bands, often resulting in grainy, low-fidelity images (e.g., \\(400 \\times 400\\) pixels).\nLumos delivers True High Definition.\nBecause our diffractive encoding is continuous and distributed, we preserve high-frequency spatial details that other systems lose. As validated in our recent Optica publication, we achieve:\n\n~1 Megapixel Resolution: \\(1304 \\times 744\\) spatial pixels.\nFull Spectral Context: 25+ bands across the Vis-NIR range.\n\nThis allows Lumos sensors to be used in applications requiring fine detail, such as detecting small defects on a production line or identifying features from high-altitude drones."
  },
  {
    "objectID": "lumos_solution.html#not-all-snapshot-sensors-are-equal",
    "href": "lumos_solution.html#not-all-snapshot-sensors-are-equal",
    "title": "The Lumos Hardware",
    "section": "Not All Snapshot Sensors Are Equal",
    "text": "Not All Snapshot Sensors Are Equal\nSome approaches use Fabry-P√©rot (Interference/Resonance) or Extended Bayer (Absorption) architectures. While these are ‚Äúsnapshot‚Äù capable, they have significant differences:\n\nSensitivity: Absorptive filters block most of the light (often &gt;90%), performing poorly in low light.\nTunnel Vision: Fabry-P√©rot filters rely on resonance. If light enters at an angle, the color shifts (blue shift). This restricts them to very narrow Fields of View (FOV) and requires telecentric lenses.\nManufacturing Risk: Resonance structures require nanometer-level tolerances, making them expensive and difficult to yield.\nCost: These systems are typically expensive to build and maintain.\nResolution: These systems are typically limited to a few hundred pixels.\n\nLumos uses Diffraction (Phase Modulation). Our optics are transparent (high sensitivity), work at wide angles (large FOV), and use robust micro-scale features that are easy to manufacture.\n\nTechnology Comparison with a Few Alternatives\n\n\n\n\n\n\n\n\n\nFeature\nLumos (Diffractive)\nFabry-P√©rot (e.g.¬†IMEC)\nExtended Bayer (5x5)\n\n\n\n\nPhysics\nPhase Modulation (Transparent)\nResonance (Interference)\nAmplitude (Absorption)\n\n\nSensitivity\nHigh (‚â•90% photons reach sensor)\nMedium (Bandwidth limited)\nLow (~1/25th of photons)\n\n\nField of View\nLarge (Standard Lenses)\nSmall (Angle Sensitive)\nLarge\n\n\nFabrication\nRobust (Micron-scale features)\nDifficult (Nanometer tolerance)\nComplex (Multi-layer pigment)"
  },
  {
    "objectID": "lumos_solution.html#the-result-the-diffractogram",
    "href": "lumos_solution.html#the-result-the-diffractogram",
    "title": "The Lumos Hardware",
    "section": "The Result: The Diffractogram",
    "text": "The Result: The Diffractogram\nThe output of our system is not a large 3D data cube as most hyperspectral systems do. It is a Diffractogram‚Äîa raw, optically compressed signal that contains the full complexity of the scene in a highly efficient format.\nThis signal is the key to our unique approach to spectral imaging.\nSee how we decode the light ‚Üí"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Spectral Intelligence, Unbound",
    "section": "",
    "text": "Smart sensing is about extracting information, not creating data.Scalable. High-Resolution. Machine-learning friendly. Efficient."
  },
  {
    "objectID": "index.html#choose-your-journey",
    "href": "index.html#choose-your-journey",
    "title": "Spectral Intelligence, Unbound",
    "section": "Choose Your Journey",
    "text": "Choose Your Journey\n\n\n\n\nüåà\n\n\nNew to Spectral Imaging?\nDiscover how the ‚ÄúChemistry of Light‚Äù is revolutionizing healthcare, agriculture, and robotics.\nExplore the Power of Light ‚Üí\n\n\n\n\n\n\nüöß\n\n\nWhy hasn‚Äôt it scaled?\nLearn about the three barriers that have kept spectral imaging mostly in the lab.\nUnderstand the Barriers ‚Üí\n\n\n\n\n\n\nüí°\n\n\nThe Lumos Solution\nSee how we combine Nanofabrication and Computational Imaging to break each barrier.\nDiscover the Tech ‚Üí"
  },
  {
    "objectID": "about_us.html",
    "href": "about_us.html",
    "title": "About Us",
    "section": "",
    "text": "Lumos Imaging is a spin-off from the University of Utah. We sit at the intersection of Nanophotonics, Applied Mathematics, and Commercial Product Strategy.\nOur mission is to translate the complex physics of diffractive optics into scalable, robust imaging solutions that solve real-world industrial problems."
  },
  {
    "objectID": "about_us.html#leadership-team",
    "href": "about_us.html#leadership-team",
    "title": "About Us",
    "section": "Leadership Team",
    "text": "Leadership Team\n\n\n\n\n\n\nRajesh Menon \n\nRajesh is a Professor at the University of Utah with a PhD in Diffractive Optics from MIT. He is a Fellow of OSA and SPIE, and a serial entrepreneur having founded LumArray and PointSpectrum. With over 100 publications and 40+ patents in nanophotonics and lithography, Rajesh brings deep technical expertise and a proven track record of translating research into commercial products.\n\n\n\n\n\n\n\n\nLaurent Node-Langlois \n\nLaurent brings over 25 years of high-tech Product Management experience. Formerly in executive leadership at GE Healthcare, where he led Medical Imaging and Surgical Navigation products, his extensive P&L responsibility includes guiding deep-tech products from R&D through market exit, bringing critical commercial execution expertise to the team.\n\n\n\n\n\n\n\n\nFernando Guevara V√°squez, PhD \n\nFernando holds a PhD in Computational & Applied Mathematics from Rice University. His research in mathematical modeling of wave propagation and inverse problems has been fundamental to developing the theoretical foundations of Lumos‚Äôs diffractive computational imaging technology. His work on optimization algorithms enables the robust reconstruction of spectral information from optically encoded data.\n\n\n\n\n\n\n\n\nFernando Gonz√°lez del Cueto, PhD \n\nFernando holds a PhD in Computational & Applied Mathematics from Rice University. His expertise in imaging science and numerical optimization has been instrumental in developing Lumos‚Äôs reconstruction algorithms and neural network architectures. His work bridges the gap between computational theory and practical implementation, creating the software foundation that makes Lumos technology viable."
  },
  {
    "objectID": "about_us.html#intellectual-property-portfolio",
    "href": "about_us.html#intellectual-property-portfolio",
    "title": "About Us",
    "section": "Intellectual Property Portfolio",
    "text": "Intellectual Property Portfolio\nLumos Imaging holds an exclusive, worldwide license to a comprehensive patent portfolio developed at the University of Utah. This IP protects the entire vertical stack‚Äîfrom the nanofabrication of the optic to the algorithmic reconstruction of the image‚Äîcreating a strong defensive moat.\n\nPatent Portfolio\n\n\n\n\n\n\n\nPatent ID\nTitle\nScope of Protection\n\n\n\n\nUS 9,723,230\nMulti-spectral imaging with diffractive optics\nHardware Architecture: Protects the physical combination of a sensor array and a diffractive element in the optical path.\n\n\nUS 11,300,449\nImaging device creating a spatially coded image\nData Format: Protects the ‚ÄúDiffractogram‚Äù data structure and the forward model logic.\n\n\nUS 8,953,239\nNanophotonic scattering structure\nDesign Algorithms: Protects the computational methods used to optimize the geometry of the DFA for specific spectral responses.\n\n\nUS 10,395,134\nExtraction of spectral information\nSoftware/Reconstruction: Protects the inverse problem algorithms used to recover the spectral cube from the raw data.\n\n\nUS 12,052,518\nMulti-modal computational imaging via metasurfaces\nAdvanced Modalities: Protects the extension of the DFA technology to other imaging modalities like Lightfield and Depth sensing."
  },
  {
    "objectID": "about_us.html#commercialization-strategic-partnerships",
    "href": "about_us.html#commercialization-strategic-partnerships",
    "title": "About Us",
    "section": "Commercialization & Strategic Partnerships",
    "text": "Commercialization & Strategic Partnerships\nWe are moving from R&D to Scale.\nLumos Imaging is actively seeking strategic partners to help us scale our technology."
  },
  {
    "objectID": "about_us.html#contact-us",
    "href": "about_us.html#contact-us",
    "title": "About Us",
    "section": "Contact Us",
    "text": "Contact Us\n\n\n\n\nEmail Us\n\n\nInterested in partnering? Reach out directly.\n\ninfo@lumosimaging.com\n\n\n\n\n\nFollow Us\n\n\nKeep up with our latest technology updates.\n\n  Lumos Imaging"
  },
  {
    "objectID": "barriers_to_HSI_adoption.html",
    "href": "barriers_to_HSI_adoption.html",
    "title": "Current Barriers to Wider Adoption of Spectral Imaging",
    "section": "",
    "text": "Traditional high-performance hyperspectral cameras rely on complex optical trains. This translates into high costs in hardware: depending on the specs, the cameras can range typically from $30k to $150k USD!\n\nComponents: These systems require precision slits, collimating mirrors, diffraction gratings, and re-focusing optics.\nAlignment: The optical path requires sub-micron alignment stability across a wide temperature range.\nConsequence: This complexity drives high unit costs ($20k‚Äì$150k USD) and makes miniaturization extremely difficult. It effectively limits HSI to high-budget research labs or military assets.\n\n\nLumos Solution: We replace complex optical trains with a single, nanofabricated diffractive optical element placed on top of an inexpensive, off-the-shelf CMOS camera. By leveraging semiconductor manufacturing techniques (Nano-Imprint Lithography), we drive the cost structure down to a level compatible with mass-market sensors."
  },
  {
    "objectID": "barriers_to_HSI_adoption.html#the-optical-complexity-barrier-cost",
    "href": "barriers_to_HSI_adoption.html#the-optical-complexity-barrier-cost",
    "title": "Current Barriers to Wider Adoption of Spectral Imaging",
    "section": "",
    "text": "Traditional high-performance hyperspectral cameras rely on complex optical trains. This translates into high costs in hardware: depending on the specs, the cameras can range typically from $30k to $150k USD!\n\nComponents: These systems require precision slits, collimating mirrors, diffraction gratings, and re-focusing optics.\nAlignment: The optical path requires sub-micron alignment stability across a wide temperature range.\nConsequence: This complexity drives high unit costs ($20k‚Äì$150k USD) and makes miniaturization extremely difficult. It effectively limits HSI to high-budget research labs or military assets.\n\n\nLumos Solution: We replace complex optical trains with a single, nanofabricated diffractive optical element placed on top of an inexpensive, off-the-shelf CMOS camera. By leveraging semiconductor manufacturing techniques (Nano-Imprint Lithography), we drive the cost structure down to a level compatible with mass-market sensors."
  },
  {
    "objectID": "barriers_to_HSI_adoption.html#the-acquisition-constraint-mechanics",
    "href": "barriers_to_HSI_adoption.html#the-acquisition-constraint-mechanics",
    "title": "Current Barriers to Wider Adoption of Spectral Imaging",
    "section": "2. The Acquisition Constraint (Mechanics)",
    "text": "2. The Acquisition Constraint (Mechanics)\n The dominant architecture for high-resolution HSI is the Push-broom (Line-scan) sensor.\n\nMechanism: The sensor captures one line at a time. To build a 3D spectral volume (\\(x, y, \\lambda\\)), the image must be stitched afterwards.\nFailure Modes:\n\nVibration: Any unmodeled vibration (e.g., from a drone or vehicle) results in ‚Äúwobbly‚Äù images that are geometrically distorted. Rectification can be a challenge.\nDynamic Scenes: If objects in the scene move during the scan, they become sheared or artifacted.\n\nConsequence: Push-broom sensors are notoriously difficult to deploy on UAVs, handheld devices, or in dynamic industrial settings.\n\n\nLumos Solution: We utilize a Snapshot architecture. We capture the full spatial-spectral volume \\((x, y, \\lambda)\\) in a single integration period. We can take pictures and video like standard cameras."
  },
  {
    "objectID": "barriers_to_HSI_adoption.html#the-dimensionality-curse-data",
    "href": "barriers_to_HSI_adoption.html#the-dimensionality-curse-data",
    "title": "Current Barriers to Wider Adoption of Spectral Imaging",
    "section": "3. The Dimensionality Curse (Data)",
    "text": "3. The Dimensionality Curse (Data)\n One understated barrier to widespread spectral adoption is the massive size of hyperpectral volumes. An uncompressed image array must be stored and processed for every spectral band.\nConsequences:\n\nSatellite Downlinks: An earth-observation satellite creates terabytes of data. Downlinking this via limited radio bandwidth is prohibitively expensive or slow. Operators often discard 90% of the data. Ironically, analysts sometimes discard a lot of these data which is unnecessary for some applications since they‚Äôre only interested in a few wavelengths, or end up combining them to an end product that is much smaller than the original datasets. Spectral data cubes are not only huge, but information is encoded very redundantly in them, making it inefficient!\nDrone Telemetry: A UAV many not be able transmit live spectral video to a ground station because the bitrate can easily exceed standard wireless protocols.\nEdge Compute: Embedded processors cannot reconstruct and analyze heavy 3D data cubes in real-time (60 FPS).\nArchival: Storing petabytes of raw hyperspectral cubes for historical analysis is very expensive.\n\nThe ‚ÄúData Tax‚Äù of Traditional Imaging In traditional systems, spectral resolution is tied to data volume. If you want 100x more spectral detail, you must transmit 100x more data. This makes real-time applications impossible.\nThe Lumos Decoupling Lumos breaks this linear relationship. Because we capture the continuous light field encoded in a single frame, our file size is constant. Whether you need 5 bands or 25 bands, the transmission cost is identical‚Äîroughly the size of a standard black-and-white photo.\nFor a standard \\(512 \\times 512\\) pixel sensor, the uncompressed data requirements vary drastically:\n\n    \n        \n            Data Explosion Calculator\n            \n            \n                \n                    Image Resolution\n                    \n                        128 x 128 px\n                        256 x 256 px\n                        512 x 512 px\n                        1024 x 1024 px\n                    \n                \n                \n                \n                    Video Duration\n                    \n                        30 Seconds\n                        1 Minute\n                        5 Minutes\n                        30 Minutes\n                    \n                \n            \n        \n\n        \n            \n\n\n\nFormat\nChannels in image\nSpectral Bands\nBits per pixel\nRelative Size\nSingle Frame\nVideo Stream\n\n\n\n\n\n\n        \n    \n        \n            * Software-Defined: Traditional cameras pay a \"data tax\" for spectral resolution‚Äîadding bands increases file size linearly. Lumos decouples storage from spectral resolution. We capture the continuous spectrum encoded in a single frame, meaning your raw transmission cost is constant regardless of how many bands you extract later. While you can mathematically define any basis (e.g., simulating 5 specific agricultural filters or a standard RGB curve), our hardware is validated to resolve ‚âà26 independent equally-spaced spectral channels in the 400‚Äì800 nm range.\n        \n  \n    \n\n\n\n\nLumos Solution: Analog Optical Compression. Spectral ‚Äúcubes‚Äù are known to be highly redundant and users of these data often throw away most of it. The Lumos Camera captures instead a lean signal that encodes spatial and spectral content in a very information-dense manner. Not to be confused with software compression, this represents a massive reduction in raw data volume, enabling the use in modern machine learning pipelines, efficient transmission from space, cloud archival and retrieval, and real-time processing for edge AI applications. Even HD video streams are possible, whereas traditional approaches are still unfeasible.\n\nLearn more about the Lumos Solution"
  },
  {
    "objectID": "lumos_algorithms.html",
    "href": "lumos_algorithms.html",
    "title": "A Paradigm Shift in Spectral Imaging: Accessible Optical Compression",
    "section": "",
    "text": "In traditional spectral imaging, the hardware dictates the data format. You are forced to capture, store, and process massive 3D data cubes, even if you only need a fraction of that information.\nLumos flips this model.\nOur hardware captures a Diffractogram‚Äîa raw, 2D grayscale image where spatial and spectral information is inextricably encoded by the laws of physics.\nThink of the Diffractogram as a ‚ÄúUniversal Container.‚Äù It is an optically compressed file (typically &lt;1MB) that holds the full complexity of the scene. Because this container is so efficient, it opens up two distinct computational pathways: Direct Inference for modern data-driven machine learning situations, and Reconstruction for traditional spectral inspection of data and analysis."
  },
  {
    "objectID": "lumos_algorithms.html#pathway-a-direct-inference",
    "href": "lumos_algorithms.html#pathway-a-direct-inference",
    "title": "A Paradigm Shift in Spectral Imaging: Accessible Optical Compression",
    "section": "Pathway A: Direct Inference",
    "text": "Pathway A: Direct Inference\n\nAccessing spectral information without spectral cubes\n‚ÄúWhy reconstruct a 50GB data cube just to answer a ‚ÄòYes/No‚Äô question?‚Äù\nFor all applications, the goal is not to see the spectrum; the goal is to act on it. A sorting facility needs to know ‚ÄúIs this glass or plastic?‚Äù A satellite needs to know ‚ÄúIs this crop stressed?‚Äù A microscope needs to know ‚ÄúIs this cell cancerous?‚Äù\nLumos enables Direct Inference from its lean data format, the diffractogram. We can use modern data-driven machine-learning frameworks and pipelines to use diffractograms directly, mapping the unique texture of the optical code to the desired answer, completely skipping the heavy reconstruction step. Unlike hyperspectral cubes which quickly become too big even for modern GPUs, diffractograms are even smaller than RGB images, making them ideal for edge devices, drones, satellites or on-demand cloud processing.\n\n\n\nDirect Inference Pipeline. Deep Learning models ingest the raw diffractogram and directly output the desired variable (Class, Abundance, etc.), skipping the reconstruction step.\n\n\n\nGamechanging advantages of Direct Inference\n\nLatency:: The small signal allows inference to take place on the edge.\nBandwidth: Ideal for drones and satellites. Transmit the answer (bytes) or the compressed diffractogram (kilobytes) instead of the raw cube (gigabytes).\nEfficiency: Eliminates redundant computation. You don‚Äôt waste energy calculating spectral bands you don‚Äôt need. Storage is efficient and on-demand processing on the cloud becomes a possibility."
  },
  {
    "objectID": "lumos_algorithms.html#pathway-b-traditional-spectral-cube-reconstruction",
    "href": "lumos_algorithms.html#pathway-b-traditional-spectral-cube-reconstruction",
    "title": "A Paradigm Shift in Spectral Imaging: Accessible Optical Compression",
    "section": "Pathway B: Traditional Spectral Cube Reconstruction",
    "text": "Pathway B: Traditional Spectral Cube Reconstruction\n\nWhen having a spectral cube makes sense\nWhen human visualization or detailed scientific analysis is required, we use our Inverse Solver algorithms to decode the diffractogram back into a spectral Cube.\nHowever, unlike traditional cameras where the bands are fixed by the hardware filters (e.g., ‚Äúyou only get these 10 bands‚Äù), Lumos offers Software-Defined Spectral Imaging. This means that we can dynamically select the bands we need after the signal has been acquired. This opens up a world of possibilities for on-demand spectral analysis.\n\n\n\n\n\n\nNoteüí° Think of it like a Prism\n\n\n\nA traditional camera slices the rainbow into fixed buckets. If you want more buckets, you need a bigger, more expensive camera. Lumos captures the ‚ÄúRainbow‚Äù as a whole. You decide where to draw the lines later.\n\n\n\n\n\nReconstruction Pipeline. The raw diffractogram is processed by the inverse solver to recover the spatio-spectral 3D cube.\n\n\n\nGamechanging advantages of Direct Inference\n\nA Posteriori Selection: You can decide after taking the picture what spectral data you need.\n\nNeed 4 bands for agriculture? We generate them.\nNeed 25 bands for geology? We generate them from the same raw file.\nNeed to simulate a specific astronomical filter set (e.g., Johnson-Cousins)? We can do that mathematically.\n\nFlexibility: The reconstruction can be done on-demand, on the entire image or on a region of interest.\nLegacy Compatibility: The output can be saved as standard .HDR or .ENVI files, making Lumos data compatible with decades of existing spectral analysis software."
  },
  {
    "objectID": "lumos_algorithms.html#workflow-comparison",
    "href": "lumos_algorithms.html#workflow-comparison",
    "title": "A Paradigm Shift in Spectral Imaging: Accessible Optical Compression",
    "section": "Workflow Comparison",
    "text": "Workflow Comparison\n\n\n\n\n\n\n\n\nFeature\nTraditional HSI Workflow\nLumos Direct Inference\n\n\n\n\nCapture\nScan scene (slow) or Snapshot (low res)\nHigh-Resolution Snapshot\n\n\nData Size\nHuge (GBs per minute)\nTiny (smaller than RGB)\n\n\nProcessing\nMust process full 3D cube\nProcess only what matters\n\n\nFlexibility\nHardware-fixed bands\nUser-defined bands, easily changed after capture\n\n\nVideo\nTypically not possible\nSame as CMOS sensor (e.g.¬†30 FPS\n\n\n\nSee our algorithms in action ‚Üí"
  },
  {
    "objectID": "validation_and_examples.html",
    "href": "validation_and_examples.html",
    "title": "Validation & Demos",
    "section": "",
    "text": "Our technology transforms standard cameras into high-definition spectral sensors. Below are validation experiments demonstrating how Lumos diffractograms capture invisible decision-making information across industrial, medical, agricultural, and scientific fields.\nLumos technology is versatile. Depending on the application, we can optimize for Speed/Efficiency (Direct Inference) or Scientific Detail (Spectral Reconstruction). Explore the examples below to see both workflows in action."
  },
  {
    "objectID": "validation_and_examples.html#pathway-a-direct-inference",
    "href": "validation_and_examples.html#pathway-a-direct-inference",
    "title": "Validation & Demos",
    "section": "Pathway A: Direct Inference",
    "text": "Pathway A: Direct Inference\nThe Efficiency Paradigm: Answers, not cubes. In these examples, we bypass the heavy generation of 3D data cubes. Algorithms extract the answer directly from the raw, compressed Diffractogram. This enables real-time performance on edge devices or efficient transmission to the cloud.\n\nIndustrial SortingAgricultural MonitoringRemote Sensing\n\n\nIndustry: Manufacturing, Recycling, and Quality Control.\nThe Problem: In high-speed sorting lines, contaminants often look identical to the product. Standard RGB cameras cannot distinguish between a clear PET plastic pellet and a clear PVC pellet, or detect a clear varnish coating on a colored object. Hyperspectral cameras can detect this, but the data volume (gigabytes per minute) is often too heavy for real-time processing.\nThe Lumos Result: In this simulation using real Lumos camera measurements, we sorted multi-colored pellets. Half were ‚Äúpure,‚Äù and half were coated with an invisible varnish.\n\nMethod: We bypassed spectral reconstruction entirely, training a classifier to run directly on the video feed.\nEfficiency: The system performed accurate classification using a stream of 0.37 MB frames, whereas a comparable 25-band spectral camera would require 15.87 MB per frame.\n\n\n\n\n\n\n\nNoteDeep Dive: The Data Advantage\n\n\n\n\n\nBy processing the diffractogram directly, we achieve a 43x reduction in data bandwidth. This allows for higher frame rates on the sorting line and reduces the computational load on the sorting robot‚Äôs embedded processor.\n\n\n\n\n\n\n\nReal-time Inference: The video shows the system correctly identifying Tainted (red) vs.¬†Pure (green) pellets despite them looking identical in RGB.Note the massive file size difference of Lumos Diffractogram vs 25-band hyperspectral cube (15.87 MB vs 0.37 MB per frame).\n\n\n\n\nIndustry: Supply Chain, Food Storage, and Phenotyping.\nThe Problem: Monitoring the ripening or spoilage of produce requires continuous observation. A standard camera can only see external color changes, which often happen too late. A hyperspectral camera could track internal chemistry (sugar/water content), but recording a continuous spectral video over days would generate unmanageable amounts of data (TB of storage).\nThe Lumos Result: We monitored an avocado rotating on a turntable every 10 minutes for 10 days.\n\nCapabilities: From the raw diffractogram data, we successfully 1) segmented the avocado from the background and 2) generated a localized estimate of the ripening index over time.\nData Volume: The full 1,000-frame video at high resolution (800x600) occupied just ~922 MB. A comparable 100-band hyperspectral video would have exploded to ~50.4 GB.\n\n\n\n\n\n\n\nNoteDeep Dive: Snapshot Advantage\n\n\n\n\n\nBecause Lumos uses a snapshot architecture, each frame is captured instantly. This allows us to image the rotating avocado without the ‚Äúshearing‚Äù or motion artifacts that would ruin the image on a traditional line-scanning (push-broom) spectral camera.\n\n\n\n\n\n\n\nLong-Duration Monitoring: The graph tracks the estimated ripening index. The snapshot capability ensures distortion-free imaging of the rotating 3D object.\n\n\n\n\nIndustry: Satellite Earth Observation and Environmental Monitoring.\nThe Problem: Satellites generate Terabytes of data, but radio downlinks are a thin pipe. Operators often have to discard vast amounts of hyperspectral data because they cannot transmit it to Earth.\nThe Lumos Result: We simulated a Lumos sensor in orbit using public Landsat 8 data. We trained a neural network to detect a specific vegetation signature.\n\nMethod: The model estimated the per-pixel abundance (0-100%) of the vegetation directly from the diffractogram.\nEfficiency: Instead of downlinking a massive data cube, the satellite could downlink the lightweight diffractogram (for ground processing) or process it on-orbit and downlink just the abundance map.\n\n\n\n\n\n\n\nNoteDeep Dive: Efficient Transmission\n\n\n\n\n\nThis experiment validates that the encoded ‚Äútexture‚Äù of the diffractogram contains sufficient information to perform sub-pixel abundance estimation. This effectively solves the downlink bottleneck, allowing small-sats to deliver hyperspectral-grade intelligence.\n\n\n\n\n\n\n\n\n\n\nInput: Raw Diffractogram (Lightweight)\n\n\n\n\n\n\n\nPrediction: Estimated Abundance\n\n\n\n\n\n\n\nGround Truth: Full Spectrum\n\n\n\n\n\n\nComparison of the predicted vegetation abundance (Center) vs.¬†the Ground Truth (Right), derived solely from the compressed input (Left)."
  },
  {
    "objectID": "validation_and_examples.html#pathway-b-spectral-reconstruction",
    "href": "validation_and_examples.html#pathway-b-spectral-reconstruction",
    "title": "Validation & Demos",
    "section": "Pathway B: Spectral Reconstruction",
    "text": "Pathway B: Spectral Reconstruction\nWe show in these examples how spectral reconstruction can be used to perform robust discrimination and prediction tasks for a wide class of problems and applications.\n\nMedical ImagingFood QualityAstronomy\n\n\nSector: Diagnostics, Surgery, Pathology, Biological Analysis.\nThe Problem: Healthy tissue and tumors, or different organ structures, often appear identical to the human eye and RGB cameras (various shades of pink). Surgeons currently rely on invasive biopsies or chemical dyes to differentiate them.\nThe Lumos Result: We imaged ex-vivo chicken tissue containing both lung and trachea.\n\nMethod: We reconstructed the spectral cube and applied a simple Linear Discriminant Analysis (LDA).\nOutcome: Despite the visual similarity, the system cleanly separated the tissues based on their spectral signatures. This proof-of-concept confirms that Lumos technology can provide the spectral discrimination necessary for biological applications.\n\n\n\n\n\n\n\nNoteDeep Dive: Learn More\n\n\n\n\n\nFor a detailed breakdown of the reconstruction accuracy and the Linear Discriminant Analysis (LDA) classification, please refer to our published paper in Optica.\n\n\n\n\n\n\n\nTissue Classification: (A) Reconstructed image with classification overlay. (B) Spectral curves showing distinct signatures for Lung vs.¬†Trachea which enabled the clean segmentation.\n\n\n\n\n\nIndustry: Supply Chain and Predictive Quality.\nThe Problem: Spoilage often begins chemically before it becomes visible. Supply chain managers need to know not just if a fruit is bad, but when it will go bad.\nThe Lumos Result: We monitored strawberries continuously over 8 days.\n\nMethod: By reconstructing the spectral cubes, we tracked the degradation of the ‚ÄúGreen Peak‚Äù (chlorophyll) and shifts in the ‚ÄúRed Edge.‚Äù\nOutcome: A regression analysis performed on the reconstructed spectra could predict the exact age of the fruit and classify ‚ÄúFresh‚Äù vs.¬†‚ÄúSpoiled‚Äù with high accuracy before visible mold appeared. This proves the reconstructed spectra has sufficient fidelity for predictive regression models.\n\n\n\n\n\nPredictive Aging: (Left) The spectral evolution of the fruit over 8 days. (Right) ROC curve demonstrating high accuracy in classifying freshness states.\n\n\n\n\n\nIndustry: Scientific Imaging and Space Domain Awareness.\nThe Problem: Traditional telescopes use mechanical filter wheels to capture different colors (e.g., Johnson-Cousins B, V, R, I filters). This is slow (sequential) and inefficient (absorptive filters block light).\nThe Lumos Result: We simulated a stellar field and captured the data in a single snapshot.\n\nSoftware-Defined Bands: Instead of using physical filters, we reconstructed the spectral response curves mathematically. We didn‚Äôt need to reconstruct 100 arbitrary bands; we solved directly for the B, V, R, and I transmission curves.\nOutcome: We recovered accurate photometric ratios from a single exposure, proving that one Lumos sensor can replicate the function of a multi-filter mechanical assembly without the moving parts.\n\n\n\n\n\n\n\nNoteDeep Dive: Basis Selection\n\n\n\n\n\nThis experiment highlights the power of A Posteriori Basis Selection. We captured the light first, then decided after the fact that we wanted to view the data through Johnson-Cousins filters. This flexibility is impossible with traditional hardware filters.\n\n\n\n\n\n\n\nSynthetic Stellar Imaging: (Top) Raw Diffractograms of three stars. (Bottom) Accurate recovery of B, V, R, and I photometric coefficients from a single exposure, matching the ground truth.\n\n\n\n\n\n\n\n\nReady to deploy this technology?\nWe are moving from R&D to commercial scale and seeking strategic partners.\nPartner with Lumos ‚Üí"
  }
]